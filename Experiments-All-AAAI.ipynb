{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80cb3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import math\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "import pickle\n",
    "from scipy.special import logit\n",
    "import shap\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "import warnings\n",
    "from shap.plots._labels import labels\n",
    "from shap.utils import format_value, ordinal_str\n",
    "from shap.plots._utils import convert_ordering, convert_color, merge_nodes, get_sort_order, sort_inds, dendrogram_coords\n",
    "from shap.plots import colors\n",
    "import scipy\n",
    "import copy\n",
    "from shap._explanation import Explanation, Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ca6ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a1e8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model\n",
    "def experiment_data_complex(p,n): \n",
    "    \n",
    "    A = np.random.binomial(1,p,n) #gender roles\n",
    "    B = [np.random.binomial(1,0.5,1)[0] if x ==0 else np.random.binomial(1,0.7,1)[0] for x in A] \n",
    "    C = [np.random.binomial(1,0.01,1)[0] if x ==0 else np.random.binomial(1,0.99,1)[0] for x in B] \n",
    "    D = [np.random.binomial(1,0.8,1)[0] if x ==0 else np.random.binomial(1,0.4,1)[0] for x in A] \n",
    "    E = [np.random.binomial(1,0.09,1)[0] if x ==0 else np.random.binomial(1,0.82,1)[0] for x in C]\n",
    "\n",
    "    df = pd.DataFrame({'A': A, 'B': B, 'C': C, 'D': D, 'E': E})\n",
    "    \n",
    "    P = 0.15*df['A'] +  0.25*df['B'] +  0.1*df['C'] + 0.2*df['D'] +  0.30*df['E']\n",
    "    \n",
    "    Y = [np.random.binomial(1,p,1)[0] for p in P] \n",
    "    df['Y'] = Y\n",
    "    \n",
    "   \n",
    "    \n",
    "    return df\n",
    "\n",
    "# Simple model\n",
    "def experiment_data_simple(p,n): \n",
    "    \n",
    "    A = np.random.binomial(1,p,n) #gender roles\n",
    "    B = [np.random.binomial(1,0.5,1)[0] if x ==0 else np.random.binomial(1,0.7,1)[0] for x in A] \n",
    "    C = [np.random.binomial(1,0.01,1)[0] if x ==0 else np.random.binomial(1,0.99,1)[0] for x in B] \n",
    "#     D = [np.random.binomial(1,0.8,1)[0] if x ==0 else np.random.binomial(1,0.4,1)[0] for x in A] \n",
    "#     E = [np.random.binomial(1,0.09,1)[0] if x ==0 else np.random.binomial(1,0.82,1)[0] for x in C]\n",
    "\n",
    "    df = pd.DataFrame({'A': A, 'B': B, 'C': C})\n",
    "    \n",
    "    P = 0.5*df['A'] +  0.2*df['B'] +  0.3*df['C']# + 0.2*df['D'] +  0.30*df['E']\n",
    "    \n",
    "    Y = [np.random.binomial(1,p,1)[0] for p in P] \n",
    "    df['Y'] = Y\n",
    "    \n",
    "   \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01237898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_data(p,w1,w2, n): # high w1 good for group 1, high w2 good for group 0, have to add up to 0.8\n",
    "    \n",
    "    df = {}\n",
    "    \n",
    "   \n",
    "    \n",
    "    A = np.random.binomial(1,p,n) #sensitive attribute\n",
    "    B1 = [np.random.binomial(1,0.2,1)[0] if x ==0 else np.random.binomial(1,0.9,1)[0] for x in A] #dependent on A\n",
    "    B2 = [np.random.binomial(1,0.2,1)[0] if x ==1 else np.random.binomial(1,0.9,1)[0] for x in A] #dependent on A\n",
    "    \n",
    "    C1 = [*np.random.binomial(1,0.5,n)]#not dependent on A\n",
    "    C2 = [*np.random.binomial(1,0.4,n)]#not dependent on A\n",
    "\n",
    "    df = pd.DataFrame({'A': A,'B1':B1,'B2':B2, 'C1': C1, 'C2': C2})\n",
    "\n",
    "    P = w1*df['B1']+ w2*df['B2'] + 0.10*df['C1']+0.10*df['C2']\n",
    " \n",
    "\n",
    "    Y = [*np.random.binomial(1,P,n)]\n",
    "#     Y=[]\n",
    "#     for p in P:\n",
    "#         if p>1:\n",
    "#             p=1\n",
    "#         y = np.random.binomial(1,p,1)\n",
    "#         Y.append(y[0])\n",
    "    df['Y'] = Y\n",
    "        \n",
    "   \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d125e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPopSD():\n",
    "    P0 = df_pop.loc[(df_pop[sensitive]==0)&(df_pop[label] ==1)].shape[0]/df_pop.loc[df_pop[sensitive]==0].shape[0]\n",
    "    P1 =df_pop.loc[(df_pop[sensitive]==1)&(df_pop[label] ==1)].shape[0]/df_pop.loc[df_pop[sensitive]==1].shape[0]\n",
    "    return P1 - P0\n",
    "def getPopRatio():\n",
    "    m0 = df_pop.loc[df_pop[sensitive]==0].shape[0]/df_pop.shape[0]\n",
    "    m1 = df_pop.loc[df_pop[sensitive]==1].shape[0]/df_pop.shape[0]\n",
    "    return m0,m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f2041dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Functions to generate samples while varying the size. \n",
    "# Used mainly for SSB experiments\n",
    "\n",
    "def sampling_sizes(sizes, df):\n",
    "    '''\n",
    "    Number of samples depend on sample size\n",
    "    '''\n",
    "\n",
    "    all_samples = {}\n",
    " \n",
    "    for size in sizes:\n",
    "        #print(size)\n",
    "        df = df.sample(frac = 1) #shuffle only for each size ONCE\n",
    "        s_num = df.shape[0]//size #number of samples depending on the sample size\n",
    "        start = [x for x in range(0,s_num*size,size)]\n",
    "        end = [x for x in range(size,s_num*size,size)]\n",
    "        samples=[]\n",
    "        for s,e in zip(start,end):\n",
    "            df = df.sample(frac = 1)\n",
    "            #print(s,e)\n",
    "            s=df.iloc[s:e]\n",
    "            samples.append(s)\n",
    "        all_samples[size] = samples\n",
    "    return all_samples\n",
    "\n",
    "def validSample(s):\n",
    "    return (s[s[label] == 0].shape[0] > 0) and (s[s[label] == 1].shape[0] > 0)\n",
    "\n",
    "def sampling_overlaping(seeds, sizes, df):\n",
    "    '''\n",
    "    Same number of samples for all sample sizes\n",
    "    '''\n",
    "    all_samples = {}\n",
    "    for size in sizes:\n",
    "        #print('size = ' + str(size))\n",
    "        samples = []\n",
    "        for seed in seeds: \n",
    "#             print('df.size: ' + str(df.size))\n",
    "#             print('sampling size: ' + str(size))\n",
    "            goodSample = False\n",
    "            attempts = 0\n",
    "            while ((goodSample == False) and (attempts < 100)):\n",
    "                s = df.sample(size, replace=True)\n",
    "                if (validSample(s)):\n",
    "                    goodSample = True\n",
    "                attempts += 1\n",
    "            #s = df.sample(size, replace=False, random_state=seed)\n",
    "            samples.append(s)\n",
    "        all_samples[size]=samples\n",
    "        \n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f2dbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the feature importance value\n",
    "def get_importance(sensitive,shap_values, max_display=10, order=Explanation.abs, clustering=None):\n",
    "    if isinstance(shap_values, Explanation):\n",
    "        cohorts = {\"\": shap_values}\n",
    "    elif isinstance(shap_values, Cohorts):\n",
    "        cohorts = shap_values.cohorts\n",
    "    else:\n",
    "        assert isinstance(shap_values,\n",
    "                          dict), \"You must pass an Explanation object, Cohorts object, or dictionary to bar plot!\"\n",
    "\n",
    "    # unpack our list of Explanation objects we need to plot\n",
    "    cohort_labels = list(cohorts.keys())\n",
    "    cohort_exps = list(cohorts.values())\n",
    "    for i in range(len(cohort_exps)):\n",
    "        if len(cohort_exps[i].shape) == 2:\n",
    "            cohort_exps[i] = cohort_exps[i].abs.mean(0)\n",
    "        assert isinstance(cohort_exps[i],\n",
    "                          Explanation), \"The shap_values paramemter must be a Explanation object, Cohorts object, or dictionary of Explanation objects!\"\n",
    "        assert cohort_exps[i].shape == cohort_exps[\n",
    "            0].shape, \"When passing several Explanation objects they must all have the same shape!\"\n",
    "        # TODO: check other attributes for equality? like feature names perhaps? probably clustering as well.\n",
    "        \n",
    "        \n",
    "    # unpack the Explanation object\n",
    "    features = cohort_exps[0].data\n",
    "    feature_names = cohort_exps[0].feature_names\n",
    "    if clustering is None:\n",
    "        partition_tree = getattr(cohort_exps[0], \"clustering\", None)\n",
    "    elif clustering is False:\n",
    "        partition_tree = None\n",
    "    else:\n",
    "        partition_tree = clustering\n",
    "    if partition_tree is not None:\n",
    "        assert partition_tree.shape[\n",
    "                   1] == 4, \"The clustering provided by the Explanation object does not seem to be a partition tree (which is all shap.plots.bar supports)!\"\n",
    "    op_history = cohort_exps[0].op_history\n",
    "    values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))])\n",
    "\n",
    "    if len(values[0]) == 0:\n",
    "        raise Exception(\"The passed Explanation is empty! (so there is nothing to plot)\")\n",
    "\n",
    "  \n",
    "\n",
    "    # TODO: Rather than just show the \"1st token\", \"2nd token\", etc. it would be better to show the \"Instance 0's 1st but\", etc\n",
    "    if issubclass(type(feature_names), str):\n",
    "        feature_names = [ordinal_str(i) + \" \" + feature_names for i in range(len(values[0]))]\n",
    "        \n",
    "        \n",
    "    # find how many instances are in each cohort (if they were created from an Explanation object)\n",
    "    cohort_sizes = []\n",
    "    for exp in cohort_exps:\n",
    "        for op in exp.op_history:\n",
    "            if op.get(\"collapsed_instances\", False):  # see if this if the first op to collapse the instances\n",
    "                cohort_sizes.append(op[\"prev_shape\"][0])\n",
    "                break\n",
    "\n",
    "    # unwrap any pandas series\n",
    "    if str(type(features)) == \"<class 'pandas.core.series.Series'>\":\n",
    "        if feature_names is None:\n",
    "            feature_names = list(features.index)\n",
    "        features = features.values\n",
    "\n",
    "    # ensure we at least have default feature names\n",
    "    if feature_names is None:\n",
    "        feature_names = np.array([labels['FEATURE'] % str(i) for i in range(len(values[0]))])\n",
    "        \n",
    "    orig_inds = [[i] for i in range(len(values[0]))]\n",
    "    orig_values = values.copy()\n",
    "\n",
    "    feature_order = np.argsort(np.mean([np.argsort(convert_ordering(order, Explanation(values[i]))) for i in range(values.shape[0])], 0))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # here we build our feature names, accounting for the fact that some features might be merged together\n",
    "    feature_inds = feature_order[:max_display]\n",
    "    y_pos = np.arange(len(feature_inds), 0, -1)\n",
    "    feature_names_new = []\n",
    "    for pos, inds in enumerate(orig_inds):\n",
    "        if len(inds) == 1:\n",
    "            feature_names_new.append(feature_names[inds[0]])\n",
    "        else:\n",
    "            full_print = \" + \".join([feature_names[i] for i in inds])\n",
    "            if len(full_print) <= 40:\n",
    "                feature_names_new.append(full_print)\n",
    "            else:\n",
    "                max_ind = np.argmax(np.abs(orig_values).mean(0)[inds])\n",
    "                feature_names_new.append(feature_names[inds[max_ind]] + \" + %d other features\" % (len(inds) - 1))\n",
    "    feature_names = feature_names_new\n",
    "\n",
    "\n",
    "    sens_ind = feature_names.index(sensitive)\n",
    "    #print(sens_ind)\n",
    "    for i in range(len(values)):\n",
    "\n",
    "        ind = feature_order[i]\n",
    "        imp = round(values[i, sens_ind],2)\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "977690c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions used in the main classification function (classifier_biases)\n",
    "def computeVariance(preds):\n",
    "    nbrSamples = len(preds)\n",
    "    sizeX = len(preds[0])\n",
    "    \n",
    "    return math.sqrt(statistics.mean([statistics.variance([float(x) for x in preds[:][i]]) for i in range(nbrSamples)]))\n",
    "\n",
    "def computeZOL(df, group):\n",
    "    \n",
    "    df_t = df.loc[(df[sensitive] == group)]\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(df_t.shape[0]):\n",
    "        currentEntry = df_t.iloc[i]\n",
    "        if (currentEntry['predictions'] != currentEntry[label]):\n",
    "            counter+=1\n",
    "    \n",
    "    return counter/df_t.shape[0]\n",
    "    \n",
    "\n",
    "def cm_metrics(y_true, y_pred, epsilon=0.0001):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    tpr = tp/(tp+fn) #true positive rate\n",
    "    fpr = fp/(fp+tn)#false positive rate\n",
    "    fnr = fn/(tp+fn)#false negative rate\n",
    "    tnr = tn/(tn+fp)\n",
    "    precision = tp/(tp+fp)\n",
    "    \n",
    "    return tpr, fpr,fnr, tnr, precision      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cbe8ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_biases(samples, X_test,Y_test, classifier, sensitiveImportance=False):\n",
    "    #classifier:\n",
    "    # rf: RandomForestClassifier()\n",
    "    # lg: LogisticRegression()\n",
    "    # lr: LinearRegression()\n",
    "    # etc.\n",
    " \n",
    "    m=0\n",
    "    \n",
    "    FPR0 = {}\n",
    "    FPR1 = {}\n",
    "    \n",
    "    FNR0 = {}\n",
    "    FNR1 = {}\n",
    "    \n",
    "    EO0 = {}\n",
    "    EO1 = {}\n",
    "    \n",
    "    SD0 = {}\n",
    "    SD1 = {}\n",
    "     \n",
    "    ZOL0 = {}\n",
    "    ZOL1 = {}\n",
    "    \n",
    "    VarFPR0 = {}\n",
    "    VarFPR1 = {}\n",
    "    \n",
    "    VarFNR0 = {}\n",
    "    VarFNR1 = {}\n",
    "    \n",
    "    VarEO0 = {}\n",
    "    VarEO1 = {}\n",
    "    \n",
    "    VarSD0 = {}\n",
    "    VarSD1 = {}\n",
    "    \n",
    "    VarZOL0 = {}\n",
    "    VarZOL1 = {}\n",
    "    \n",
    "    VarDisc_FPR = {}\n",
    "    VarDisc_EO = {}\n",
    "    VarDisc_ZOL = {}\n",
    "    VarDisc_AUC = {}\n",
    "    VarDisc_SD = {}\n",
    "    \n",
    "    fpr_roc_data0 = {}\n",
    "    fpr_roc_data1 = {}\n",
    "    \n",
    "    tpr_roc_data0 = {}\n",
    "    tpr_roc_data1 = {}\n",
    "    \n",
    "    auc0 = {}\n",
    "    auc1 = {} \n",
    "    \n",
    "    Var_auc0 = {}\n",
    "    Var_auc1 = {}\n",
    "    \n",
    "    shapVal = {}\n",
    "    varShapVal = {}\n",
    "       \n",
    "    max_count = len(samples)\n",
    "    f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "    display(f) # display the bar\n",
    "    \n",
    "    for key,value in samples.items(): # loop over sizes\n",
    "        f.value += 1\n",
    "        \n",
    "        fpr0_temp = []\n",
    "        fpr1_temp = []\n",
    "        #fpr_temp = []\n",
    "        \n",
    "        fnr0_temp = []\n",
    "        fnr1_temp = []\n",
    "        #fnr_temp = []\n",
    "        \n",
    "        eo0_temp = []\n",
    "        eo1_temp = []\n",
    "        \n",
    "        sd0_temp = []\n",
    "        sd1_temp = []\n",
    "        #sd_temp = []\n",
    "        \n",
    "        zol0_temp = []\n",
    "        zol1_temp = []\n",
    "        \n",
    "        fpr0_roc_temp = []\n",
    "        fpr1_roc_temp = []\n",
    "        tpr0_roc_temp = []\n",
    "        tpr1_roc_temp = []\n",
    "        \n",
    "        auc0_temp = []\n",
    "        auc1_temp = []  \n",
    "              \n",
    "        shapVal_temp = []\n",
    "        \n",
    "        Disc_FPR_temp = []\n",
    "        Disc_ZOL_temp = []\n",
    "        Disc_EO_temp = []\n",
    "        Disc_AUC_temp = []\n",
    "        Disc_SD_temp = []\n",
    "       \n",
    "        \n",
    "        \n",
    "        for v in value: # loop over samples of the same size\n",
    "            data=v\n",
    "            m = len(data)\n",
    "            X = data.drop(label, axis=1)\n",
    "            Y = data[label]\n",
    "            \n",
    "            if (classifier == 'lg'):\n",
    "                cl = LogisticRegression()\n",
    "            elif classifier=='dt':\n",
    "                cl=tree.DecisionTreeClassifier()\n",
    "            elif (classifier == 'rf'):\n",
    "                cl = RandomForestClassifier()\n",
    "            elif (classifier == 'svm'):\n",
    "                cl = svm.SVC()\n",
    "                #cl = svm.LinearSVC()\n",
    "            elif (classifier == 'nnc'):\n",
    "                cl = KNeighborsClassifier(n_neighbors=3)\n",
    "            elif (classifier == 'nb'):\n",
    "                cl = GaussianNB()\n",
    "            elif (classifier == 'nnet'):\n",
    "                cl = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "            else:\n",
    "                print('error')\n",
    "                exit()\n",
    "            \n",
    "            model=cl.fit(X,Y)\n",
    "            predictions = model.predict(X_test)\n",
    "            \n",
    "            df_temp = X_test.copy()\n",
    "            df_temp['predictions'] = predictions  \n",
    "            df_temp[label] = Y_test\n",
    "            \n",
    "            if (classifier == 'svm'):\n",
    "                # since auc is not possible with svm, put anything:\n",
    "                fpr_roc_1 = [0.0,0.0]\n",
    "                tpr_roc_1 = [0.0,0.0]\n",
    "                auc_1 = 0.0\n",
    "                fpr_roc_0 = [0.0,0.0]\n",
    "                tpr_roc_0 = [0.0,0.0]\n",
    "                auc_0 = 0.0\n",
    "            else:\n",
    "                y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "                df_temp['pred_proba'] = y_pred_proba\n",
    "                \n",
    "                fpr_roc, tpr_roc, _ = metrics.roc_curve(Y_test,  y_pred_proba)\n",
    "\n",
    "                #df_temp_X_1 = df_temp.loc[(df_temp[sensitive]==1)].drop(label, axis=1)\n",
    "                df_temp_Y_1 = df_temp.loc[(df_temp[sensitive]==1)][label]\n",
    "                df_temp_pred_proba_1 = df_temp.loc[(df_temp[sensitive]==1)]['pred_proba']\n",
    "                fpr_roc_1, tpr_roc_1, _ = metrics.roc_curve(df_temp_Y_1,  df_temp_pred_proba_1)\n",
    "                auc_1 = metrics.roc_auc_score(df_temp_Y_1,  df_temp_pred_proba_1)\n",
    "\n",
    "                df_temp_Y_0 = df_temp.loc[(df_temp[sensitive]==0)][label]\n",
    "                df_temp_pred_proba_0 = df_temp.loc[(df_temp[sensitive]==0)]['pred_proba']\n",
    "                fpr_roc_0, tpr_roc_0, _ = metrics.roc_curve(df_temp_Y_0,  df_temp_pred_proba_0)\n",
    "                auc_0 = metrics.roc_auc_score(df_temp_Y_0,  df_temp_pred_proba_0)\n",
    "#                \n",
    "            fpr0 = df_temp.loc[(df_temp[sensitive]==0)&(df_temp.predictions ==1)&(df_temp[label] == 0)].shape[0]/\\\n",
    "                    df_temp.loc[(df_temp[sensitive]==0)&(df_temp[label] == 0)].shape[0]  \n",
    "            fpr1 = df_temp.loc[(df_temp[sensitive]==1)&(df_temp.predictions ==1)&(df_temp[label] == 0)].shape[0]/\\\n",
    "                    df_temp.loc[(df_temp[sensitive]==1)&(df_temp[label] == 0)].shape[0]\n",
    "#             fpr = df_temp.loc[(df_temp.predictions ==1)&(df_temp[label] == 0)].shape[0]/\\\n",
    "#                     df_temp.loc[(df_temp[label] == 0)].shape[0]\n",
    "            \n",
    "            fnr0 = df_temp.loc[(df_temp[sensitive]==0)&(df_temp.predictions ==0)&(df_temp[label] == 1)].shape[0]/\\\n",
    "                    df_temp.loc[(df_temp[sensitive]==0)&(df_temp[label] == 1)].shape[0]  \n",
    "            fnr1 = df_temp.loc[(df_temp[sensitive]==1)&(df_temp.predictions ==0)&(df_temp[label] == 1)].shape[0]/\\\n",
    "                    df_temp.loc[(df_temp[sensitive]==1)&(df_temp[label] == 1)].shape[0]\n",
    "            \n",
    "            eo0 = df_temp.loc[(df_temp[sensitive]==0)&(df_temp.predictions ==1)&(df_temp[label] == 1)].shape[0]/\\\n",
    "                    df_temp.loc[(df_temp[sensitive]==0)&(df_temp[label] == 1)].shape[0]  \n",
    "            eo1 = df_temp.loc[(df_temp[sensitive]==1)&(df_temp.predictions ==1)&(df_temp[label] == 1)].shape[0]/\\\n",
    "                    df_temp.loc[(df_temp[sensitive]==1)&(df_temp[label] == 1)].shape[0]            \n",
    "            \n",
    "            sd0 = df_temp.loc[(df_temp[sensitive]==0)&(df_temp.predictions ==1)].shape[0]/\\\n",
    "            df_temp.loc[df_temp[sensitive]==0].shape[0]\n",
    "            sd1 = df_temp.loc[(df_temp[sensitive]==1)&(df_temp.predictions ==1)].shape[0]/\\\n",
    "            df_temp.loc[df_temp[sensitive]==1].shape[0]\n",
    "            \n",
    "            g0 = df_temp.loc[(df_temp[sensitive] == 0)]\n",
    "            zol0 = (g0['predictions'] ^ g0[label]).sum()/g0.shape[0]\n",
    "            \n",
    "            g1 = df_temp.loc[(df_temp[sensitive] == 1)]\n",
    "            zol1 = (g1['predictions'] ^ g1[label]).sum()/g1.shape[0]\n",
    "           \n",
    "            fpr0_temp.append(fpr0);\n",
    "            fpr1_temp.append(fpr1);\n",
    "           \n",
    "            fnr0_temp.append(fnr0);\n",
    "            fnr1_temp.append(fnr1);\n",
    "            \n",
    "            eo0_temp.append(eo0)\n",
    "            eo1_temp.append(eo1)\n",
    "            \n",
    "            sd0_temp.append(sd0)\n",
    "            sd1_temp.append(sd1)\n",
    "            \n",
    "            zol0_temp.append(zol0)\n",
    "            zol1_temp.append(zol1)\n",
    "            \n",
    "            fpr0_roc_temp.append(fpr_roc_0)\n",
    "            fpr1_roc_temp.append(fpr_roc_1)\n",
    "            tpr0_roc_temp.append(tpr_roc_0)\n",
    "            tpr1_roc_temp.append(tpr_roc_1)\n",
    "            \n",
    "            auc0_temp.append(auc_0)\n",
    "            auc1_temp.append(auc_1)\n",
    "            \n",
    "            Disc_FPR_temp.append(fpr1 - fpr0)\n",
    "            Disc_ZOL_temp.append(zol1 - zol0)\n",
    "            Disc_EO_temp.append(eo1 - eo0)\n",
    "            Disc_AUC_temp.append(auc_1 - auc_0)\n",
    "            Disc_SD_temp.append(sd1 - sd0)\n",
    "            \n",
    "            # Leaving the following block here means that feature importance is computed\n",
    "            # for every sample. This will take a lot of time. For samples larger than 200\n",
    "            # This will take inacceptable amount of time. Instead, it should be commented \n",
    "            # and the feature importance is computed only once (further below)\n",
    "            \n",
    "            if (sensitiveImportance):# and (classifier != 'svm')):\n",
    "                explainer = shap.explainers.Exact(model.predict, X)\n",
    "                shap_values = explainer(X)\n",
    "                shapVal_temp.append(get_importance(sensitive,shap_values))\n",
    "            else:\n",
    "                shapVal_temp.append(0.0)\n",
    "                \n",
    "         \n",
    "       \n",
    "        FPR0[key] = statistics.mean(fpr0_temp)\n",
    "        FPR1[key] = statistics.mean(fpr1_temp)\n",
    "        \n",
    "        FNR0[key] = statistics.mean(fnr0_temp)\n",
    "        FNR1[key] = statistics.mean(fnr1_temp)\n",
    "\n",
    "        EO0[key] = statistics.mean(eo0_temp)\n",
    "        EO1[key] = statistics.mean(eo1_temp)\n",
    "\n",
    "        SD0[key] = statistics.mean(sd0_temp)\n",
    "        SD1[key] = statistics.mean(sd1_temp)\n",
    "        \n",
    "        ZOL0[key] = statistics.mean(zol0_temp)\n",
    "        ZOL1[key] = statistics.mean(zol1_temp)\n",
    "        \n",
    "        VarFPR0[key] = statistics.variance(fpr0_temp)\n",
    "        VarFPR1[key] = statistics.variance(fpr1_temp)\n",
    "        \n",
    "        VarFNR0[key] = statistics.variance(fnr0_temp)\n",
    "        VarFNR1[key] = statistics.variance(fnr1_temp)\n",
    "        \n",
    "        VarEO0[key] = statistics.variance(eo0_temp)\n",
    "        VarEO1[key] = statistics.variance(eo1_temp)\n",
    "        \n",
    "        VarSD0[key] = statistics.variance(sd0_temp)\n",
    "        VarSD1[key] = statistics.variance(sd1_temp)\n",
    "        \n",
    "        VarZOL0[key] = statistics.variance(zol0_temp)\n",
    "        VarZOL1[key] = statistics.variance(zol1_temp)\n",
    "        \n",
    "        fpr_roc_data0[key] = roc_mean(fpr0_roc_temp)\n",
    "        fpr_roc_data1[key] = roc_mean(fpr1_roc_temp)\n",
    "        \n",
    "        tpr_roc_data0[key] = roc_mean(tpr0_roc_temp)\n",
    "        tpr_roc_data1[key] = roc_mean(tpr1_roc_temp)\n",
    "        \n",
    "        auc0[key] = statistics.mean(auc0_temp)\n",
    "        auc1[key] = statistics.mean(auc1_temp)\n",
    "        \n",
    "        Var_auc0[key] = statistics.variance(auc0_temp)\n",
    "        Var_auc1[key] = statistics.variance(auc1_temp)\n",
    "        \n",
    "        shapVal[key] = statistics.mean(shapVal_temp)\n",
    "        varShapVal[key] = statistics.variance(shapVal_temp)\n",
    "        \n",
    "        # If feature importance is computed only for one sample,\n",
    "        # comment the above block about feature importance and\n",
    "        # uncomment the below block.\n",
    "        \n",
    "#         if (sensitiveImportance): # and (classifier != 'svm')):\n",
    "#             explainer = shap.explainers.Exact(model.predict, X)\n",
    "#             shap_values = explainer(X)\n",
    "#             shapVal[key] = get_importance(sensitive,shap_values)\n",
    "#         else:\n",
    "#             shapVal[key] = 0.0\n",
    "        \n",
    "#         varShapVal[key] = 0.0\n",
    "        \n",
    "        VarDisc_FPR[key] = statistics.variance(Disc_FPR_temp)\n",
    "        VarDisc_ZOL[key] = statistics.variance(Disc_ZOL_temp)\n",
    "        VarDisc_EO[key] = statistics.variance(Disc_EO_temp)\n",
    "        VarDisc_AUC[key] = statistics.variance(Disc_AUC_temp)\n",
    "        VarDisc_SD[key] = statistics.variance(Disc_SD_temp)\n",
    "    \n",
    "       \n",
    "    \n",
    "    return FPR0, FPR1, FNR0, FNR1, EO0, EO1, SD0, SD1, ZOL0, ZOL1, VarFPR0, VarFPR1,\\\n",
    "            VarFNR0, VarFNR1, VarEO0, VarEO1, VarSD0, VarSD1, VarZOL0, VarZOL1, \\\n",
    "            fpr_roc_data0, fpr_roc_data1, tpr_roc_data0, tpr_roc_data1, auc0, auc1, \\\n",
    "            Var_auc0, Var_auc1, shapVal, varShapVal, \\\n",
    "            VarDisc_FPR, VarDisc_EO, VarDisc_ZOL, VarDisc_AUC, VarDisc_SD\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "320b154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_mean(roc_data):\n",
    "    #Get the smallest list\n",
    "    minLength = math.inf\n",
    "    for i in range(len(roc_data)):\n",
    "        if (len(roc_data[i]) < minLength):\n",
    "            minLength = len(roc_data[i])\n",
    "    \n",
    "    avg_roc = []\n",
    "    for i in range(minLength):\n",
    "        tot = 0\n",
    "        for j in range(len(roc_data)):\n",
    "            tot += roc_data[j][i]\n",
    "        avg_roc.append(tot / len(roc_data))\n",
    "    \n",
    "    return avg_roc\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0261b487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40ad00af9e24d4a968f64fd371a5ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell is for testing purposes for the classifier_biases function.\n",
    "\n",
    "dataset = 'Adult'\n",
    "\n",
    "if (dataset == 'synthetic-complex'):\n",
    "    df_pop_origin = experiment_data_complex(0.5,30000)\n",
    "    dataset = 'synthetic-complex'\n",
    "    df_pop = df_pop_origin\n",
    "    sensitive = 'A'\n",
    "    label = 'Y'\n",
    "elif (dataset == 'synthetic-simple'):\n",
    "    df_pop_origin = experiment_data_simple(0.5,30000)\n",
    "    dataset = 'synthetic-simple'\n",
    "    df_pop = df_pop_origin\n",
    "    sensitive = 'A'\n",
    "    label = 'Y'\n",
    "elif (dataset == 'synthetic-new'):\n",
    "    p=0.5 #proportion of each sensitive group\n",
    "    n=30000 #data size\n",
    "    w1,w2 = (0.6,0.2) #equal contribution (have to add up to 0.8)\n",
    "    df_pop_origin = class_data(p,w1,w2, n)\n",
    "    df_pop = df_pop_origin\n",
    "    sensitive = 'A'\n",
    "    label = 'Y'\n",
    "elif (dataset == 'Adult'):\n",
    "    df_pop_origin = pd.read_csv('Adult/adult_cleaned_bin.csv')\n",
    "    df_pop = df_pop_origin\n",
    "    sensitive = 'Sex'\n",
    "    label = 'income'\n",
    "elif (dataset == 'Compas'):\n",
    "    df_pop_origin = pd.read_csv('Compas/compas_simplified.csv')\n",
    "    df_pop = df_pop_origin\n",
    "    dataset = 'Compas'\n",
    "    sensitive = 'race'\n",
    "    label = 'two_year_recid'\n",
    "elif (dataset == 'Dutch'):\n",
    "    df_pop_origin = pd.read_csv('Dutch/dutch_simplified.csv')\n",
    "    df_pop = df_pop_origin\n",
    "    dataset = 'Dutch'\n",
    "    sensitive = 'sex'\n",
    "    label = 'occupation'\n",
    "elif (dataset == 'German'):\n",
    "    df_pop_origin = pd.read_csv('German/german_simplified.csv')\n",
    "    df_pop = df_pop_origin\n",
    "    dataset = 'German'\n",
    "    sensitive = 'personal_status_sex'\n",
    "    label = 'default'\n",
    "elif (dataset == 'Default'):\n",
    "    df_pop_origin = pd.read_csv('Default/UCI_Credit_Card_Processed.csv')\n",
    "    df_pop = df_pop_origin.replace({'SEX': {1:0, 2:1}})\n",
    "    dataset = 'Default'\n",
    "    sensitive = 'SEX'\n",
    "    label = 'DEF_PAY_NMO'\n",
    "    \n",
    "\n",
    "y = df_pop[label]\n",
    "X = df_pop.drop(label, axis = 1)\n",
    "\n",
    "pop_train, pop_test, y_poptrain, y_poptest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "pop_train[label]=y_poptrain\n",
    "\n",
    "nbrSamples = 100\n",
    "# seeds = [x for x in range(nbrSamples)]\n",
    "# sizes = [x for x in range(10, 50, 5)] + [x for x in range(50, 100, 10)] + \\\n",
    "#         [x for x in range(100, 1000, 100)] +\\\n",
    "#         [x for x in range(1000, 9000, 1000)]\n",
    "seeds = [1,2]\n",
    "sizes = [100,200]\n",
    "\n",
    "\n",
    "# proportionsf = [x for x in np.arange(0.000001,0.02,0.002)] + \\\n",
    "#                 [round(x/10,3) for x in np.arange(0.2,10,0.5)] + \\\n",
    "#                 [x for x in np.arange(0.98,0.9999,0.002)]\n",
    "# proportionsf = proportionsf[1:]\n",
    "\n",
    "#classifiers = ['lg','rf','nnc','nb','svm'] \n",
    "classifier = 'lg'\n",
    "\n",
    "samples = sampling_overlaping(seeds, sizes, pop_train)\n",
    "\n",
    "FPR0, FPR1, FNR0, FNR1, EO0, EO1, SD0, SD1, ZOL0, ZOL1, VarFPR0, VarFPR1,\\\n",
    "            VarFNR0, VarFNR1, VarEO0, VarEO1, VarSD0, VarSD1, VarZOL0, VarZOL1, \\\n",
    "            fpr_roc_data0, fpr_roc_data1, tpr_roc_data0, tpr_roc_data1, auc0, auc1, \\\n",
    "            Var_auc0, Var_auc1, shapVal, varShapVal, \\\n",
    "            VarDisc_FPR, VarDisc_EO, VarDisc_ZOL, VarDisc_AUC, VarDisc_SD\\\n",
    "            = classifier_biases(samples, pop_test,y_poptest, classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187a2ea",
   "metadata": {},
   "source": [
    "## Cross Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd077392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling functions for the URB and data augmentation experiments\n",
    "\n",
    "# Sampling function for URB experiments\n",
    "def sample_ratios(size, df, n):\n",
    "    all_samples={}\n",
    "    proportionsf = [round(x/10,2) for x in np.arange(0.2,10,0.2)]\n",
    "    females=df[df[sensitive]==0].copy()\n",
    "    males=df[df[sensitive]==1].copy()\n",
    "    for p in proportionsf:\n",
    "        samples=[]\n",
    "        for i in range(n):\n",
    "            f = females.sample(frac = 1)#shuffle\n",
    "            #f = f.sample(frac = 1)#shuffle\n",
    "            fn= round(size*p)\n",
    "            sf = f.iloc[0:fn]\n",
    "            m = males.sample(frac = 1)#shuffle\n",
    "            m = m.sample(frac = 1)#shuffle\n",
    "            fm=size-fn\n",
    "            sm = m.iloc[0:fm]\n",
    "            s=pd.concat([sm, sf], ignore_index=True, axis=0)\n",
    "            samples.append(s)\n",
    "        all_samples[p]=samples\n",
    "    \n",
    "    return all_samples\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def generateSamples(size, df, proportion, n):\n",
    "    samples = []\n",
    "    p = proportion\n",
    "        \n",
    "    for i in range(n):\n",
    "            females=df[df[sensitive]==0].copy().sample(frac = 1)\n",
    "            males=df[df[sensitive]==1].copy().sample(frac = 1)\n",
    "            goodSample = False\n",
    "            attempts = 0\n",
    "            while ((goodSample == False) and (attempts < 100)):\n",
    "                sf = females.sample(round(p*size), replace=True)\n",
    "                sm = males.sample(round(size-(p*size)), replace=True)\n",
    "                if (validSample(sf) and validSample(sm)):\n",
    "                    goodSample = True\n",
    "                    #print('attempt: ' + str(attempts))\n",
    "                attempts += 1\n",
    "            s=pd.concat([sm, sf], ignore_index=True, axis=0)\n",
    "            #s = s.sample(frac = 1)\n",
    "            samples.append(s)\n",
    "    return samples\n",
    "\n",
    "def sample_ratios_overlap(size, df, proportionsf, n):\n",
    "    all_samples={}\n",
    "    for p in proportionsf:\n",
    "        all_samples[p] = generateSamples(size,df,p,n)\n",
    "    \n",
    "    return all_samples\n",
    "\n",
    "# Fixing the privileged group size while changing the protected group size (randomly)\n",
    "def sample_for_threshold(sizeG1, df, sizesG0, n):\n",
    "    all_samples={}\n",
    "    g1=df[df[sensitive]==1].copy().sample(frac = 1)\n",
    "    g0=df[df[sensitive]==0].copy().sample(frac = 1)\n",
    "    \n",
    "    for sizeG0 in sizesG0:\n",
    "        all_samples[sizeG0] = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        s1 = g1.sample(sizeG1, replace=True)\n",
    "        for sizeG0 in sizesG0:\n",
    "            s0 = g0.sample(sizeG0, replace=True)\n",
    "            s = pd.concat([s1, s0], ignore_index=True, axis=0)\n",
    "            s = s.sample(frac = 1)\n",
    "            all_samples[sizeG0].append(s)\n",
    "    \n",
    "    return all_samples\n",
    "\n",
    "# Fixing the privileged group size while changing the protected group size \n",
    "# by selecting only positive outcome samples from the protected group.\n",
    "def sample_for_threshold_selective(sizeG1, df, sizesG0, n):\n",
    "    all_samples={}\n",
    "    g1=df[df[sensitive]==1].copy().sample(frac = 1)\n",
    "    g0=df[df[sensitive]==0].copy().sample(frac = 1)\n",
    "    g0p=df.loc[(df[sensitive]==0) & (df[label]==1)].copy().sample(frac = 1)\n",
    "    #df_temp.loc[(df_temp[sensitive]==0)&(df_temp[label] == 1)]\n",
    "    \n",
    "    firstSize = sizesG0[0]\n",
    "    \n",
    "    for sizeG0 in sizesG0:\n",
    "        all_samples[sizeG0] = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        s1 = g1.sample(sizeG1, replace=True)\n",
    "        for sizeG0 in sizesG0:\n",
    "            s0 = g0.sample(firstSize, replace=True)\n",
    "            s0p = g0p.sample(sizeG0 - firstSize, replace = True)\n",
    "            s = pd.concat([s1, s0, s0p], ignore_index=True, axis=0)\n",
    "            s = s.sample(frac = 1)\n",
    "            all_samples[sizeG0].append(s)\n",
    "    \n",
    "    return all_samples\n",
    "\n",
    "# A different way to select samples.\n",
    "def sample_for_threshold_v2(sizeG1, df, sizesG0, n):\n",
    "    all_samples={}\n",
    "    \n",
    "#     females=df[df[sensitive]==0].copy()\n",
    "#     males=df[df[sensitive]==1].copy()\n",
    "    for sizeG0 in sizesG0:\n",
    "        samples = []\n",
    "        for i in range(n):\n",
    "            g1=df[df[sensitive]==1].copy().sample(frac = 1)\n",
    "            g0=df[df[sensitive]==0].copy().sample(frac = 1)\n",
    "            s1 = g1.sample(sizeG1, replace=True)\n",
    "            s0 = g0.sample(sizeG0, replace=True)\n",
    "            s = pd.concat([s1, s0], ignore_index=True, axis=0)\n",
    "            s = s.sample(frac = 1)\n",
    "            samples.append(s)            \n",
    "        \n",
    "        all_samples[sizeG0] = samples\n",
    "    \n",
    "    return all_samples\n",
    "\n",
    "# Fixing the protected group size while changing the privileged group size\n",
    "def sample_for_threshold_inv(sizesG1, df, sizeG0, n):\n",
    "    all_samples={}\n",
    "    g1=df[df[sensitive]==1].copy().sample(frac = 1)\n",
    "    g0=df[df[sensitive]==0].copy().sample(frac = 1)\n",
    "    \n",
    "    for sizeG1 in sizesG1:\n",
    "        all_samples[sizeG1] = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        s0 = g0.sample(sizeG0, replace=True)\n",
    "        for sizeG1 in sizesG1:\n",
    "            s1 = g1.sample(sizeG1, replace=True)\n",
    "            s = pd.concat([s1, s0], ignore_index=True, axis=0)\n",
    "            s = s.sample(frac = 1)\n",
    "            all_samples[sizeG1].append(s)\n",
    "    \n",
    "    return all_samples\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions used in genDataCV function\n",
    "def computeAverageFolds(key, m_list):\n",
    "\n",
    "    nbrFolds = len(m_list)\n",
    "    tot = 0\n",
    "    for k in range(nbrFolds):\n",
    "        tot += m_list[k][key]\n",
    "    return (tot/nbrFolds)\n",
    "\n",
    "def computeAverageRocs(key, roc_list):\n",
    "    nbrFolds = len(roc_list)\n",
    "    minLength = math.inf\n",
    "    for j in range(nbrFolds):\n",
    "        if (len(roc_list[j][key]) < minLength):\n",
    "            minLength = len(roc_list[j][key])\n",
    "    \n",
    "    roc_list_avg = []\n",
    "\n",
    "    for j in range(minLength):\n",
    "        tot = 0\n",
    "        for i in range(nbrFolds):\n",
    "            tot += roc_list[i][key][j]\n",
    "        roc_list_avg.append(tot/nbrFolds)\n",
    "    \n",
    "    return roc_list_avg\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb491f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function for cross-validation. It generates the samples and calls classifier_biases function\n",
    "def genDataCV(folds, m, sizes, seeds, classifier, task, sizeG = 0, sensitiveImportance=False):\n",
    "    \n",
    "    # task:\n",
    "    # sizes\n",
    "    # ratios\n",
    "    \n",
    "    \n",
    "    df_pop_shuffule = df_pop.sample(frac=1)\n",
    "    y = df_pop_shuffule[label]\n",
    "    X = df_pop_shuffule.drop(label, axis = 1)\n",
    "    kf = KFold(n_splits=folds)\n",
    "    nbrFolds = kf.get_n_splits(X)\n",
    "    \n",
    "    nbrSamples = len(seeds)\n",
    "    \n",
    "    fpr0_folds = {}\n",
    "    fpr1_folds = {}\n",
    "    fnr0_folds = {}\n",
    "    fnr1_folds = {}\n",
    "    eo0_folds = {}\n",
    "    eo1_folds = {}\n",
    "    sd0_folds = {}\n",
    "    sd1_folds = {}\n",
    "    zol0_folds = {}\n",
    "    zol1_folds = {}\n",
    "    var_fpr0_folds = {}\n",
    "    var_fpr1_folds = {}\n",
    "    var_fnr0_folds = {}\n",
    "    var_fnr1_folds = {}\n",
    "    var_eo0_folds = {}\n",
    "    var_eo1_folds = {}\n",
    "    var_sd0_folds = {}\n",
    "    var_sd1_folds = {}\n",
    "    var_zol0_folds = {}\n",
    "    var_zol1_folds = {}\n",
    "    \n",
    "    fpr_roc0_folds = {}\n",
    "    fpr_roc1_folds = {}\n",
    "    tpr_roc0_folds = {}\n",
    "    tpr_roc1_folds = {}\n",
    "    auc0_folds = {}\n",
    "    auc1_folds = {}\n",
    "    shapVal_folds = {}\n",
    "    var_auc0_folds = {}\n",
    "    var_auc1_folds = {}\n",
    "    var_shapVal_folds = {}\n",
    "    \n",
    "    var_disc_fpr_folds = {}\n",
    "    var_disc_eo_folds = {}\n",
    "    var_disc_zol_folds = {}\n",
    "    var_disc_auc_folds = {}\n",
    "    var_disc_sd_folds = {}\n",
    "    \n",
    "    fpr0_list = []\n",
    "    fpr1_list = []\n",
    "    fnr0_list = []\n",
    "    fnr1_list = []\n",
    "    eo0_list = []\n",
    "    eo1_list = []\n",
    "    sd0_list = []\n",
    "    sd1_list = []\n",
    "    zol0_list = []\n",
    "    zol1_list = []\n",
    "    var_fpr0_list = []\n",
    "    var_fpr1_list = []\n",
    "    var_fnr0_list = []\n",
    "    var_fnr1_list = []\n",
    "    var_eo0_list = []\n",
    "    var_eo1_list = []\n",
    "    var_sd0_list = []\n",
    "    var_sd1_list = []\n",
    "    var_zol0_list = []\n",
    "    var_zol1_list = []\n",
    "    \n",
    "    var_disc_fpr_list = []\n",
    "    var_disc_eo_list = []\n",
    "    var_disc_zol_list = []\n",
    "    var_disc_auc_list = []\n",
    "    var_disc_sd_list = []\n",
    "    \n",
    "    fpr_roc0_list = []\n",
    "    fpr_roc1_list = []\n",
    "    tpr_roc0_list = []\n",
    "    tpr_roc1_list = []\n",
    "    auc0_list = []\n",
    "    auc1_list = []\n",
    "    shapVal_list = []\n",
    "    var_auc0_list = []\n",
    "    var_auc1_list = []\n",
    "    var_shapVal_list = []\n",
    "    \n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        pop_train = df_pop_shuffule.iloc[train_index]\n",
    "        pop_test = df_pop_shuffule.iloc[test_index]\n",
    "        y_test = pop_test[label]\n",
    "        X_test = pop_test.drop(label, axis=1)\n",
    "        #print(X_test)\n",
    "        \n",
    "        if (task == 'sizes'):\n",
    "            samples = sampling_overlaping(seeds, sizes, pop_train)\n",
    "        elif (task == 'ratios'):\n",
    "            samples = sample_ratios_overlap(m, pop_train, sizes, nbrSamples)\n",
    "        elif (task == 'threshold'):\n",
    "            # Here, you select which sampling approach is used\n",
    "            # (depends on which experiment is being performed).\n",
    "            samples = sample_for_threshold(sizeG,pop_train,sizes, nbrSamples)\n",
    "            #samples = sample_for_threshold_selective(sizeG,pop_train,sizes, nbrSamples)\n",
    "            #samples = sample_for_threshold_inv(sizes,pop_train,sizeG, nbrSamples)\n",
    "            \n",
    "        \n",
    "        FPR0, FPR1, FNR0, FNR1, EO0, EO1, SD0, SD1, ZOL0, ZOL1, VarFPR0, VarFPR1,\\\n",
    "            VarFNR0, VarFNR1, VarEO0, VarEO1, VarSD0, VarSD1, VarZOL0, VarZOL1, \\\n",
    "            fpr_roc0, fpr_roc1, tpr_roc0, tpr_roc1, auc0, auc1, \\\n",
    "            Var_auc0, Var_auc1, shapVal, varShapVal, \\\n",
    "            VarDisc_FPR, VarDisc_EO, VarDisc_ZOL, VarDisc_AUC, VarDisc_SD = \\\n",
    "            classifier_biases(samples, X_test, y_test, classifier,\\\n",
    "                              sensitiveImportance=sensitiveImportance)\n",
    "        \n",
    "\n",
    "        fpr0_list.append(FPR0)\n",
    "        fpr1_list.append(FPR1)\n",
    "        fnr0_list.append(FNR0)\n",
    "        fnr1_list.append(FNR1)\n",
    "        eo0_list.append(EO0)\n",
    "        eo1_list.append(EO1)\n",
    "        sd0_list.append(SD0)\n",
    "        sd1_list.append(SD1)\n",
    "        zol0_list.append(ZOL0)\n",
    "        zol1_list.append(ZOL1)\n",
    "        var_fpr0_list.append(VarFPR0)\n",
    "        var_fpr1_list.append(VarFPR1)\n",
    "        var_fnr0_list.append(VarFNR0)\n",
    "        var_fnr1_list.append(VarFNR1)\n",
    "        var_eo0_list.append(VarEO0)\n",
    "        var_eo1_list.append(VarEO1)\n",
    "        var_sd0_list.append(VarSD0)\n",
    "        var_sd1_list.append(VarSD1)\n",
    "        var_zol0_list.append(VarZOL0)\n",
    "        var_zol1_list.append(VarZOL1)\n",
    "        fpr_roc0_list.append(fpr_roc0)\n",
    "        fpr_roc1_list.append(fpr_roc1)\n",
    "        tpr_roc0_list.append(tpr_roc0)\n",
    "        tpr_roc1_list.append(tpr_roc1)\n",
    "        auc0_list.append(auc0)\n",
    "        auc1_list.append(auc1)\n",
    "        shapVal_list.append(shapVal)\n",
    "        var_auc0_list.append(Var_auc0)\n",
    "        var_auc1_list.append(Var_auc1)\n",
    "        var_shapVal_list.append(varShapVal)\n",
    "        var_disc_fpr_list.append(VarDisc_FPR)\n",
    "        var_disc_eo_list.append(VarDisc_EO)\n",
    "        var_disc_zol_list.append(VarDisc_ZOL)\n",
    "        var_disc_auc_list.append(VarDisc_AUC)\n",
    "        var_disc_sd_list.append(VarDisc_SD)\n",
    "  \n",
    "    for key in list(FPR0.keys()):\n",
    "        \n",
    "        fpr0_folds[key] = computeAverageFolds(key, fpr0_list) \n",
    "        fpr1_folds[key] = computeAverageFolds(key, fpr1_list)\n",
    "        fnr0_folds[key] = computeAverageFolds(key, fnr0_list)\n",
    "        fnr1_folds[key] = computeAverageFolds(key, fnr1_list)\n",
    "        eo0_folds[key] = computeAverageFolds(key, eo0_list)\n",
    "        eo1_folds[key] = computeAverageFolds(key, eo1_list)\n",
    "        sd0_folds[key] = computeAverageFolds(key, sd0_list)\n",
    "        sd1_folds[key] = computeAverageFolds(key, sd1_list)\n",
    "        zol0_folds[key] = computeAverageFolds(key, zol0_list)\n",
    "        zol1_folds[key] = computeAverageFolds(key, zol1_list)\n",
    "        \n",
    "\n",
    "        var_fpr0_folds[key] = computeAverageFolds(key, var_fpr0_list)\n",
    "        var_fpr1_folds[key] = computeAverageFolds(key, var_fpr1_list)\n",
    "        var_fnr0_folds[key] = computeAverageFolds(key, var_fnr0_list)\n",
    "        var_fnr1_folds[key] = computeAverageFolds(key, var_fnr1_list)\n",
    "        var_eo0_folds[key] = computeAverageFolds(key, var_eo0_list)\n",
    "        var_eo1_folds[key] = computeAverageFolds(key, var_eo1_list)\n",
    "        var_sd0_folds[key] = computeAverageFolds(key, var_sd0_list)\n",
    "        var_sd1_folds[key] = computeAverageFolds(key, var_sd1_list)\n",
    "        var_zol0_folds[key] = computeAverageFolds(key, var_zol0_list)\n",
    "        var_zol1_folds[key] = computeAverageFolds(key, var_zol1_list)\n",
    "                \n",
    "        fpr_roc0_folds[key] = computeAverageRocs(key, fpr_roc0_list)\n",
    "        fpr_roc1_folds[key] = computeAverageRocs(key, fpr_roc1_list)\n",
    "        tpr_roc0_folds[key] = computeAverageRocs(key, tpr_roc0_list)\n",
    "        tpr_roc1_folds[key] = computeAverageRocs(key, tpr_roc1_list)\n",
    "        auc0_folds[key] = computeAverageFolds(key, auc0_list)\n",
    "        auc1_folds[key] = computeAverageFolds(key, auc1_list)\n",
    "        shapVal_folds[key] = computeAverageFolds(key, shapVal_list)\n",
    "        var_auc0_folds[key] = computeAverageFolds(key, var_auc0_list)\n",
    "        var_auc1_folds[key] = computeAverageFolds(key, var_auc1_list)\n",
    "        var_shapVal_folds[key] = computeAverageFolds(key, var_shapVal_list)\n",
    "        \n",
    "        var_disc_fpr_folds[key] = computeAverageFolds(key, var_disc_fpr_list)\n",
    "        var_disc_eo_folds[key] = computeAverageFolds(key, var_disc_eo_list)\n",
    "        var_disc_zol_folds[key] = computeAverageFolds(key, var_disc_zol_list)\n",
    "        var_disc_auc_folds[key] = computeAverageFolds(key, var_disc_auc_list)\n",
    "        var_disc_sd_folds[key] = computeAverageFolds(key, var_disc_sd_list)\n",
    "        \n",
    "    return fpr0_folds, fpr1_folds, fnr0_folds, fnr1_folds, eo0_folds, eo1_folds, sd0_folds, sd1_folds,\\\n",
    "            zol0_folds, zol1_folds, var_fpr0_folds, var_fpr1_folds, var_fnr0_folds, var_fnr1_folds, \\\n",
    "            var_eo0_folds, var_eo1_folds, var_sd0_folds, var_sd1_folds, var_zol0_folds, var_zol1_folds,\\\n",
    "            fpr_roc0_folds, fpr_roc1_folds, tpr_roc0_folds, tpr_roc1_folds, auc0_folds, auc1_folds, \\\n",
    "            var_auc0_folds, var_auc1_folds, shapVal_folds, var_shapVal_folds, \\\n",
    "            var_disc_fpr_folds, var_disc_eo_folds, var_disc_zol_folds, var_disc_auc_folds, var_disc_sd_folds\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cc8010c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2785e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_SSB_URB(meas0, meas1, ref0, ref1):\n",
    "\n",
    "    experiments = list(meas0.keys())\n",
    "\n",
    "    SSURB = {}\n",
    "    Disc = {}\n",
    "    disc_ref = ref1 - ref0\n",
    "    \n",
    "    for k in experiments:\n",
    "        Disc[k] = meas1[k] - meas0[k]\n",
    "        SSURB[k] = meas1[k] - meas0[k] - disc_ref       \n",
    "\n",
    "    return Disc, disc_ref, SSURB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f659d5",
   "metadata": {},
   "source": [
    "# Main script for all experiments in the paper\n",
    "This script carries out experiment and save the obtained data structures (mainly dictionary) in files.\n",
    "These files can be retrieved for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50955b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the parameters for the desired experiment:\n",
    "\n",
    "# Select which experiment to perform (task):\n",
    "# sizes: SSB experiment\n",
    "# ratios: URB experiment\n",
    "# threshold: Data augmentation experiment\n",
    "\n",
    "task = 'threshold'\n",
    "\n",
    "#dataset = 'Dutch'\n",
    "#datasets = ['synthetic-complex', 'synthetic-simple', 'Adult-bin','Adult-not-bin','Compas','Dutch']#,'German']\n",
    "#datasets = ['Adult-bin','Compas','Dutch', 'Adult-not-bin','synthetic-complex', 'synthetic-simple', 'synthetic-new']#,'German']\n",
    "datasets = ['Dutch', 'Adult-bin']\n",
    "\n",
    "# Whether to save the results in files\n",
    "fileSaving = True\n",
    "\n",
    "# Size of the training set for the URB experiment\n",
    "m = 1000\n",
    "\n",
    "# How many times each instance of an experiment is repeated\n",
    "# (Finally the average of all those experiments is kept)\n",
    "# (This will also allow to compute variance)\n",
    "nbrSamples = 50\n",
    "seeds = [x for x in range(nbrSamples)]\n",
    "\n",
    "# Sample sizes for the SSB experiment\n",
    "#     sizes = [x for x in range(10, 50, 5)] + [x for x in range(50, 100, 10)] + \\\n",
    "#             [x for x in range(100, 1000, 100)] +\\\n",
    "#             [x for x in range(1000, 9000, 1000)]\n",
    "sizes = [15, 20, 25, 30, 35, 40, 45, 60, 80, 100, \\\n",
    "         120, 140, 160, 210, 260, 310, 360, 410, \\\n",
    "         460, 540, 640, 740, 840, 940, 1080, 1280, \\\n",
    "         1480, 1680, 1880, 2150, 2650, 3150, 3650, \\\n",
    "         4150, 4650, 5400, 6400, 7400, 8400]\n",
    "\n",
    "\n",
    "# Sizes for the data augmentation experiment\n",
    "sizesThreshold = [x for x in range(1, 100, 2)]\n",
    "\n",
    "# Size of the portion (typically privileged group) to stay fixed in the data augmentation experiment\n",
    "singleSize = 100\n",
    "\n",
    "# Proportions for the URB experiment\n",
    "#proportionsf = [round(x/10,2) for x in np.arange(0.2,10,0.2)]\n",
    "proportionsf = [x for x in np.arange(0.000001,0.02,0.002)] + \\\n",
    "                [round(x/10,3) for x in np.arange(0.2,10,0.5)] + \\\n",
    "                [x for x in np.arange(0.98,0.9999,0.002)]\n",
    "proportionsf = proportionsf[1:]\n",
    "\n",
    "\n",
    "# Select the classifiers to be used\n",
    "#classifiers = ['lg','dt','rf','nnc','nb','svm'] \n",
    "classifiers = ['lg','dt']#,'rf','nnc', 'svm'] \n",
    "#classifiers = ['lg']\n",
    "\n",
    "# Number of folds for the cross-validation\n",
    "nbrFolds = 3\n",
    "\n",
    "# Estimate sensitive feature importance using SHAP\n",
    "# Warning: this can slow experiments significantly\n",
    "computeImportance = False\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "\n",
    "    if (dataset == 'synthetic-complex'):\n",
    "        df_pop_origin = experiment_data_complex(0.5,30000)\n",
    "        dataset = 'synthetic-complex'\n",
    "        df_pop = df_pop_origin\n",
    "        sensitive = 'A'\n",
    "        label = 'Y'\n",
    "    elif (dataset == 'synthetic-simple'):\n",
    "        df_pop_origin = experiment_data_simple(0.5,30000)\n",
    "        dataset = 'synthetic-simple'\n",
    "        df_pop = df_pop_origin\n",
    "        sensitive = 'A'\n",
    "        label = 'Y'\n",
    "    elif (dataset == 'synthetic-new'):\n",
    "        p=0.5 #proportion of each sensitive group\n",
    "        n=30000 #data size\n",
    "        w1,w2 = (0.6,0.2) #equal contribution (have to add up to 0.8)\n",
    "        df_pop_origin = class_data(p,w1,w2, n)\n",
    "        df_pop = df_pop_origin\n",
    "        sensitive = 'A'\n",
    "        label = 'Y'\n",
    "    elif (dataset == 'Adult-bin'):\n",
    "        df_pop_origin = pd.read_csv('Adult/adult_cleaned_bin.csv')\n",
    "        df_pop = df_pop_origin\n",
    "        sensitive = 'Sex'\n",
    "        label = 'income'\n",
    "    elif (dataset == 'Adult-not-bin'):\n",
    "        df_pop_origin = pd.read_csv('Adult/adult_cleaned_not_bin.csv')\n",
    "        df_pop = df_pop_origin\n",
    "        sensitive = 'Sex'\n",
    "        label = 'income'\n",
    "    elif (dataset == 'Compas'):\n",
    "        df_pop_origin = pd.read_csv('Compas/compas_simplified.csv')\n",
    "        df_pop = df_pop_origin\n",
    "        dataset = 'Compas'\n",
    "        sensitive = 'race'\n",
    "        label = 'two_year_recid'\n",
    "    elif (dataset == 'Dutch'):\n",
    "        df_pop_origin = pd.read_csv('Dutch/dutch_simplified.csv')\n",
    "        df_pop = df_pop_origin\n",
    "        dataset = 'Dutch'\n",
    "        sensitive = 'sex'\n",
    "        label = 'occupation'\n",
    "    elif (dataset == 'German'):\n",
    "        df_pop_origin = pd.read_csv('German/german_simplified.csv')\n",
    "        df_pop = df_pop_origin\n",
    "        dataset = 'German'\n",
    "        sensitive = 'personal_status_sex'\n",
    "        label = 'default'\n",
    "\n",
    "\n",
    "    y = df_pop[label]\n",
    "    X = df_pop.drop(label, axis = 1)\n",
    "    pop_train, pop_test, y_poptrain, y_poptest = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    pop_train[label]=y_poptrain\n",
    "\n",
    "\n",
    "\n",
    "    if(task == 'sizes'):\n",
    "        experiments = sizes\n",
    "    if(task == 'ratios'):\n",
    "        experiments = proportionsf\n",
    "    if(task == 'threshold'):\n",
    "        experiments = sizesThreshold\n",
    "\n",
    "    for i in range(len(classifiers)):   \n",
    "   \n",
    "        classifier = classifiers[i]\n",
    "    \n",
    "        if (task == 'sizes'):\n",
    "            print( 'nbr Samples = ' + str(nbrSamples) + ', classifier: ' + classifier +\n",
    "              ', nbr Folds = ' + str(nbrFolds))\n",
    "        elif (task == 'ratios'):    \n",
    "            print( 'nbr Samples = ' + str(nbrSamples) + ', classifier: ' + classifier + \\\n",
    "                  ', sample size = ' + str(m) + ', nbr Folds = ' + str(nbrFolds))\n",
    "        elif (task == 'threshold'):    \n",
    "            print( 'nbr Samples = ' + str(nbrSamples) + ', classifier: ' + classifier + \\\n",
    "                  ', G1 size = ' + str(singleSize) + ', nbr Folds = ' + str(nbrFolds))\n",
    "\n",
    "        fpr0, fpr1, fnr0, fnr1, eo0, eo1, sd0, sd1,\\\n",
    "                zol0, zol1, var_fpr0, var_fpr1, var_fnr0, var_fnr1, \\\n",
    "                var_eo0, var_eo1, var_sd0, var_sd1, var_zol0, var_zol1,\\\n",
    "                fpr_roc0, fpr_roc1, tpr_roc0, tpr_roc1, auc0, auc1, \\\n",
    "                var_auc0, var_auc1, shapVal, var_shapVal,\\\n",
    "                var_disc_fpr, var_disc_eo, var_disc_zol, var_disc_auc, var_disc_sd= \\\n",
    "            genDataCV(nbrFolds, m, experiments, seeds, classifier, task, \\\n",
    "                      sizeG = singleSize, sensitiveImportance=computeImportance)\n",
    "\n",
    "        if (task == 'sizes' or task == 'threshold'):\n",
    "            ref = math.floor(df_pop.shape[0] * ((nbrFolds-1)/nbrFolds))\n",
    "        if (task == 'ratios'):\n",
    "            fr,mr = getPopRatio() \n",
    "            ref = fr\n",
    "\n",
    "\n",
    "        experimentRef = [ref]\n",
    "        if (task == 'threshold'):\n",
    "            taskTh = 'sizes'\n",
    "        else:\n",
    "            taskTh = task\n",
    "\n",
    "        # Compute the same quantities for the reference scenario\n",
    "        # Reference is used mainly for SSB and URB experiments\n",
    "        fpr0_ref_d, fpr1_ref_d, fnr0_ref_d, fnr1_ref_d, eo0_ref_d, eo1_ref_d, sd0_ref_d, sd1_ref_d,\\\n",
    "        zol0_ref_d, zol1_ref_d, var_fpr0_ref_d, var_fpr1_ref_d, var_fnr0_ref_d, var_fnr1_ref_d, \\\n",
    "        var_eo0_ref_d, var_eo1_ref_d, var_sd0_ref_d, var_sd1_ref_d, var_zol0_ref_d, var_zol1_ref_d, \\\n",
    "        fpr_roc0_ref_d, fpr_roc1_ref_d, tpr_roc0_ref_d, tpr_roc1_ref_d, auc0_ref_d, auc1_ref_d, \\\n",
    "        var_auc0_ref_d, var_auc1_ref_d, shapVal_ref_d, var_shapVal_ref_d,\\\n",
    "        var_disc_fpr_ref_d, var_disc_eo_ref_d, var_disc_zol_ref_d, var_disc_auc_ref_d, var_disc_sd_ref_d = \\\n",
    "                    genDataCV(nbrFolds, m, experimentRef, seeds, classifier, taskTh, sizeG = singleSize)\n",
    "\n",
    "        fpr0_ref = fpr0_ref_d[ref]; fpr1_ref = fpr1_ref_d[ref]; fnr0_ref = fnr0_ref_d[ref]; \n",
    "        fnr1_ref = fnr1_ref_d[ref]; eo0_ref = eo0_ref_d[ref]; eo1_ref = eo1_ref_d[ref];\n",
    "        sd0_ref = sd0_ref_d[ref]; sd1_ref = sd1_ref_d[ref]; zol0_ref = zol0_ref_d[ref]; zol1_ref = zol1_ref_d[ref]\n",
    "        var_fpr0_ref = var_fpr0_ref_d[ref];var_fpr1_ref = var_fpr1_ref_d[ref];\n",
    "        var_fnr0_ref = var_fnr0_ref_d[ref];var_fnr1_ref = var_fnr1_ref_d[ref];\n",
    "        var_eo0_ref = var_eo0_ref_d[ref];var_eo1_ref = var_eo1_ref_d[ref];\n",
    "        var_sd0_ref = var_sd0_ref_d[ref];var_sd1_ref = var_sd1_ref_d[ref];\n",
    "        var_zol0_ref = var_zol0_ref_d[ref];var_zol1_ref = var_zol1_ref_d[ref];\n",
    "        \n",
    "        fpr_roc0_ref = fpr_roc0_ref_d[ref]; fpr_roc1_ref = fpr_roc1_ref_d[ref];\n",
    "        tpr_roc0_ref = tpr_roc0_ref_d[ref]; tpr_roc1_ref = tpr_roc1_ref_d[ref];\n",
    "        auc0_ref = auc0_ref_d[ref]; auc1_ref = auc1_ref_d[ref]; var_auc0_ref = var_auc0_ref_d[ref];\\\n",
    "        var_auc1_ref = var_auc1_ref_d[ref]; shapVal_ref = shapVal_ref_d[ref]; var_shapVal_ref = var_shapVal_ref_d[ref]\n",
    "        var_disc_fpr_ref = var_disc_fpr_ref_d[ref]; var_disc_eo_ref = var_disc_eo_ref_d[ref];\n",
    "        var_disc_zol_ref = var_disc_zol_ref_d[ref]; var_disc_auc_ref = var_disc_auc_ref_d[ref];\n",
    "        var_disc_sd_ref = var_disc_sd_ref_d[ref]\n",
    "\n",
    "        D_fpr, D_fpr_ref, SSURB_fpr = compute_SSB_URB(fpr0, fpr1, fpr0_ref, fpr1_ref)\n",
    "        D_fnr, D_fnr_ref, SSURB_fnr = compute_SSB_URB(fnr0, fnr1, fnr0_ref, fnr1_ref)\n",
    "        D_eo, D_eo_ref, SSURB_eo = compute_SSB_URB(eo0, eo1, eo0_ref, eo1_ref)\n",
    "        D_sd, D_sd_ref, SSURB_sd = compute_SSB_URB(sd0, sd1, sd0_ref, sd1_ref)\n",
    "        D_zol, D_zol_ref, SSURB_zol = compute_SSB_URB(zol0, zol1, zol0_ref, zol1_ref)\n",
    "        D_var_fpr, D_var_fpr_ref, SSURB_var_fpr = compute_SSB_URB(var_fpr0, var_fpr1, \\\n",
    "                                                                  var_fpr0_ref, var_fpr1_ref)\n",
    "        D_var_fnr, D_var_fnr_ref, SSURB_var_fnr = compute_SSB_URB(var_fnr0, var_fnr1, \\\n",
    "                                                                  var_fnr0_ref, var_fnr1_ref)\n",
    "        D_var_eo, D_var_eo_ref, SSURB_var_eo = compute_SSB_URB(var_eo0, var_eo1, \\\n",
    "                                                               var_eo0_ref, var_eo1_ref)\n",
    "        D_var_sd, D_var_sd_ref, SSURB_var_sd = compute_SSB_URB(var_sd0, var_sd1, \\\n",
    "                                                               var_sd0_ref, var_sd1_ref)\n",
    "        D_var_zol, D_var_zol_ref, SSURB_var_zol = compute_SSB_URB(var_zol0, var_zol1, \\\n",
    "                                                                  var_zol0_ref, var_zol1_ref)\n",
    "\n",
    "   \n",
    "        if (fileSaving):\n",
    "            currentDirectory = '/Users/sami/GitHub/RepresentationBias/savedResults/'\n",
    "            folderName = time.strftime('%Y%m%d-%H%M%S-' + dataset + '-' + classifier + '-' + task + '-sensContr-30/')\n",
    "            os.mkdir(currentDirectory + folderName)\n",
    "            if (task == 'ratios'):\n",
    "                filename = 'ratiosData-'  + str(m) + '-' + classifier + '.pkl'\n",
    "            if (task == 'sizes'):\n",
    "                filename = 'sizesData-' + classifier + '.pkl'\n",
    "            if (task == 'threshold'):\n",
    "                filename = 'thresholdData-' + classifier + '.pkl'\n",
    "            fullfilename = currentDirectory + folderName + filename \n",
    "\n",
    "            plottingData = (fpr0, fpr1, fnr0, fnr1, eo0, eo1, \\\n",
    "                            sd0, sd1, zol0, zol1,\n",
    "                            var_fpr0, var_fpr1, var_fnr0, var_fnr1, var_eo0,\\\n",
    "                            var_eo1, var_sd0, var_sd1, var_zol0, var_zol1, \\\n",
    "                            fpr_roc0, fpr_roc1, tpr_roc0, tpr_roc1, auc0, auc1,\\\n",
    "                            var_auc0, var_auc1, shapVal, var_shapVal,\\\n",
    "                            var_disc_fpr, var_disc_eo, var_disc_zol, var_disc_auc, var_disc_sd,\\\n",
    "                            fpr0_ref, fpr1_ref, fnr0_ref, fnr1_ref, eo0_ref, eo1_ref, sd0_ref, sd1_ref,\\\n",
    "                            zol0_ref, zol1_ref, var_fpr0_ref, var_fpr1_ref, var_fnr0_ref, var_fnr1_ref, \\\n",
    "                            var_eo0_ref, var_eo1_ref, var_sd0_ref, var_sd1_ref, var_zol0_ref, var_zol1_ref, \\\n",
    "                            fpr_roc0_ref, fpr_roc1_ref, tpr_roc0_ref, tpr_roc1_ref, auc0_ref, auc1_ref, \\\n",
    "                            var_auc0_ref, var_auc1_ref, shapVal_ref, var_shapVal_ref, \\\n",
    "                            var_disc_fpr_ref, var_disc_eo_ref, var_disc_zol_ref, var_disc_auc_ref, var_disc_sd_ref, \\\n",
    "                            task, nbrSamples, dataset, classifier)\n",
    "\n",
    "            with open(fullfilename, 'wb') as f:\n",
    "                pickle.dump(plottingData, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
